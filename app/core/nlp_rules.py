# app/core/nlp_rules.py
from pydoc import text
import json, os, re
from difflib import SequenceMatcher

DATA_DIR = os.path.join('app', 'data')
SYNONYMS_FILE = os.path.join(DATA_DIR, 'synonyms.json')



# -------------------------------------------------------------
# INTENCI칍N GENERAL
# -------------------------------------------------------------
def detect_intent(text: str) -> str:
    text = text.lower()
    if any(k in text for k in ['precio', 'cu치nto', 'cotiza', 'total', 'cuenta']):
        return 'quote'
    if any(k in text for k in ['tiempo', 'entrega', 'm칤nimo', 'pago', 'invima', 'certificado']):
        return 'faq'
    return 'other'


# -------------------------------------------------------------
# INTENCI칍N DE COMPRA
# -------------------------------------------------------------
def detect_purchase_intent(text: str) -> str:
    text = text.lower()

    high_intent = [
        "env칤ame", "hazme la cuenta", "quiero pedir", "cot칤zame",
        "necesito para", "urgente", "m치ndame la cotizaci칩n",
        "c칩mo te pago", "cu치nto me sale", "ya tengo pedido"
    ]

    medium_intent = [
        "me interesa", "cu치nto vale", "qu칠 precio tiene",
        "pueden enviar", "cu치nto demora", "quiero saber si tienen",
        "podr칤an cotizarme", "estoy mirando precios"
    ]

    if any(p in text for p in high_intent):
        return "high"
    elif any(p in text for p in medium_intent):
        intent = "medium"
    else:
        intent = "low"

    # Detecci칩n de pedidos grandes
    if re.search(r'(\b\d+\s*(unidades?|cajas?|bultos?|litros?|kilos?|sacos?)\b|\bpedido grande\b|\ben cantidad\b)', text):
        return "high"

    return intent


# -------------------------------------------------------------
# INTENCI칍N LOG칈STICA
# -------------------------------------------------------------
def detect_logistics_intent(text: str) -> tuple[bool, dict]:
    """
    Detecta si el mensaje se refiere a temas log칤sticos (entrega, cobertura, etc.).
    Retorna (True/False, {"type": str, "city": Optional[str]}).
    """
    if not text:
        return False, {}

    import unicodedata

    text = text.lower().strip()
    text = unicodedata.normalize("NFKD", text)
    text = "".join(c for c in text if not unicodedata.combining(c))
    text = text.replace("", "").replace("?", "").replace("춰", "").replace("!", "")

    logistics_keywords = [
        r"\b(entrega|entregan|entregar|entregado|entregas)\b",
        r"\b(envio|envian|enviar|enviarlo|envios)\b",
        r"\b(despacho|despachos|despachan|despachar)\b",
        r"\b(reparto|repartos|domicilio|domicilios|mensajeria|repartidor)\b",
        r"\b(cobertura|cubren|alcance)\b",
        r"\b(horario|hora|horas|ma침ana|tarde|noche|noches|fines?\s+de\s+semana|sabados?|domingos?)\b"
    ]

    if not any(re.search(pat, text) for pat in logistics_keywords):
        return False, {}

    # Tipificaci칩n log칤stica
    if re.search(r"\b(fines?\s+de\s+semana|sabados?|domingos?)\b", text):
        subtype = "weekend"
    elif re.search(r"\b(horario|hora|horas|ma침ana|tarde|noche|noches)\b", text):
        subtype = "time_window"
    elif re.search(r"\b(cobertura|cubren|alcance|otras?\s+ciudades|fuera|nacional|envian\s+a)\b", text):
        subtype = "coverage"
    elif re.search(r"\b(cuanto\s+tardan?|tiempos?\s+de\s+entrega|plazo)\b", text):
        subtype = "delivery_time"
    else:
        subtype = "generic"

    city_match = re.search(
        r"\b(en|a)\s+(bogota|medellin|cali|barranquilla|cartagena|bucaramanga|pereira|manizales|cucuta)\b",
        text,
    )
    city = city_match.group(2).title() if city_match else None
    if city and subtype == "generic":
        subtype = "city_delivery"

    return True, {"type": subtype, "city": city}


# -------------------------------------------------------------
# NORMALIZACI칍N MULTIPRODUCTO
# -------------------------------------------------------------
def normalize_input(text: str) -> list[str]:
    """
    Busca todos los productos mencionados en el texto.
    Devuelve lista con nombres can칩nicos encontrados.
    Soporta plurales, errores menores y coincidencias parciales.
    """
    import unicodedata
    from difflib import SequenceMatcher

    # Cargar sin칩nimos
    try:
        with open(SYNONYMS_FILE, encoding='utf-8') as f:
            synonyms = json.load(f)
    except Exception:
        synonyms = {}

    # Normalizar texto (acentos y espacios)
    msg = text.lower().strip()
    msg = unicodedata.normalize("NFKD", msg)
    msg = "".join(c for c in msg if not unicodedata.combining(c))

    encontrados = set()

    for canonical, variants in synonyms.items():
        for v in variants:
            term = v.lower().strip()
            term = unicodedata.normalize("NFKD", term)
            term = "".join(c for c in term if not unicodedata.combining(c))

            # Coincidencia directa
            if term in msg:
                encontrados.add(canonical)
                continue

            # Coincidencia por palabras (una o m치s)
            if all(w in msg for w in term.split()):
                encontrados.add(canonical)
                continue

            # Coincidencia difusa (por similitud)
            ratio = SequenceMatcher(None, term, msg).ratio()
            if ratio > 0.65:
                encontrados.add(canonical)

    return list(encontrados)


# -------------------------------------------------------------
# INTENCIONES ADICIONALES
# -------------------------------------------------------------
def detect_additional_intents(text: str) -> dict:
    """
    Detecta intenciones adicionales: FAQ, discount_info, should_escalate.
    Prioridad: should_escalate > logistics > faq > discount.
    """
    text = text.lower()
    intents = {"faq": False, "discount_info": False, "should_escalate": False}

    # --- FAQ detection ---
    faq_keywords = [
        "m칤nimo", "minimos", "compra m칤nima", "pedido m칤nimo",
        "forma de pago", "formas de pago", "pago", "pagos",
        "contraentrega", "efectivo", "tarjeta", "cr칠dito", "d칠bito",
        "devoluci칩n", "devoluciones", "cambio", "cambios",
        "reembolso", "reembolsos", "tiempo de entrega", "entregan",
        "cu치nto se demora la entrega", "disponibilidad", "stock", "existencias",
        "da침ado", "mal olor", "defectuoso", "combinar", "mezclar", "mismo pedido",
        "certificado", "invima", "iva"
    ]
    if any(k in text for k in faq_keywords):
        intents["faq"] = True

    # --- Discounts detection ---
    discount_keywords = [
        "promocion", "promoci칩n", "oferta", "descuento", "descuentos",
        "rebaja", "promo", "en oferta"
    ]
    if any(k in text for k in discount_keywords):
        intents["discount_info"] = True

    # --- Escalation detection ---
    escalate_keywords = [
        "reclamo", "problema", "queja", "error", "equivocado",
        "confusi칩n", "pedido incorrecto", "producto equivocado",
        "pedido incompleto", "demora", "retraso", "no ha llegado", "todav칤a no llega",
        "repartidor", "cobrado", "cobro incorrecto", "precio distinto",
        "olvid칩", "olvido", "esperando", "falta", "da침ado", "cambio", "incompleto otra vez"
    ]

    if any(k in text for k in escalate_keywords):
        intents["should_escalate"] = True

    # --- Priority rules ---
    if intents["should_escalate"]:
        intents["faq"] = False
        intents["discount_info"] = False

    # --- Safe overrides ---
    # Frases informativas que nunca deben escalar
    safe_keywords = ["invima", "certificado invima", "iva", "descuento", "promoci칩n", "oferta", "certificado"]
    if any(sk in text for sk in safe_keywords):
        intents["should_escalate"] = False
        intents["faq"] = True

    return intents

# --- Extraer m칰ltiples productos y cantidades ---
def extract_products_and_quantities(message: str) -> list[dict]:
    import json, os, re, unicodedata
    from rapidfuzz import fuzz  # Aseg칰rate de tener instalado rapidfuzz

    # --- Definir normalizaci칩n ---
    def norm(s: str) -> str:
        s = s.lower().strip()
        s = unicodedata.normalize("NFKD", s)
        return "".join(c for c in s if not unicodedata.combining(c))

    def strip_accents(s: str) -> str:
        s = unicodedata.normalize("NFKD", s)
        return "".join(c for c in s if not unicodedata.combining(c))

    txt = norm(message or "")
    txt = strip_accents(txt)

    # --- Normalizaci칩n r치pida antes del match ---
    txt = txt.lower()
    txt = txt.replace(";", ",").replace("+", ",").replace("/", ",")
    txt = txt.replace(" y ", ",").replace(" e ", ",").replace(" con ", ",")
    txt = re.sub(r'(?<=\d)(?=[a-z치칠칤칩칰침])', ' ', txt)
    txt = re.sub(r'(?<=[a-z치칠칤칩칰침])(?=\d)', ' ', txt)  # separa 8leches -> 8 leches
    txt = re.sub(r'\s+', ' ', txt).strip(',')
    txt = re.sub(r'(?<=\d)(?=[a-z])', ' ', txt)  # separa 9mm -> 9 mm

    DATA_DIR = os.path.join("app", "data")
    SYNONYMS_FILE = os.path.join(DATA_DIR, "synonyms.json")

    # --- Cargar sin칩nimos ---
    try:
        with open(SYNONYMS_FILE, encoding="utf-8") as f:
            synonyms = json.load(f)
    except Exception:
        return []

    print("DEBUG TXT:", txt)

    found_items = []

    # 游댳 Crear mapa enriquecido con plurales autom치ticos
    enriched = {}
    for canonical, variants in synonyms.items():
        sset = set()
        for v in variants:
            base = strip_accents(v.lower().strip())
            sset.add(base)
            if not base.endswith("s"):
                sset.add(base + "s")
            else:
                sset.add(base[:-1])
        enriched[canonical] = list(sset)

    # --- Reparar productos pegados sin espacios ---
    for canonical, variants in enriched.items():
        for v in variants:
            compact = v.replace(" ", "")
            if compact in txt:
                txt = txt.replace(compact, v)

    # 游댳 Buscar cantidades + sin칩nimo dentro del texto completo
    for canonical, variants in enriched.items():
        qty_total = 0
        matched = False

        # --- Coincidencia exacta ---
        for variant in variants:
            pattern = rf"(?<![a-z치칠칤칩칰침])(\d+)\s+(?:de\s+)?{re.escape(variant)}(?:\s*9\s*mm)?(?![\w치칠칤칩칰침])"
            matches = list(re.finditer(pattern, txt))
            if matches:
                qty_total = int(matches[0].group(1))  # toma solo la primera coincidencia
                matched = True
                break  # evita contar duplicados

        # --- Coincidencia difusa (si no hubo match exacto) ---
        if not matched:
            tokens = txt.split()
            for token in tokens:
                for variant in variants:
                    if fuzz.ratio(token, variant) >= 80:
                        qty_total = 1
                        matched = True
                        break
                if matched:
                    break

        # --- Agregar si hubo match ---
        if matched:
            found_items.append({
                "nombre": canonical,
                "cantidad": qty_total
            })

    # 游댳 Ordenar seg칰n posici칩n en el texto original (para coherencia en la respuesta)
    found_items.sort(
        key=lambda i: min(
            (txt.find(v) for v in enriched.get(i["nombre"], []) if txt.find(v) != -1),
            default=9999
        )
    )

    # 游댳 Si hay productos sin n칰mero expl칤cito, contar 1 (solo si no se detect칩 ya con cantidad)
    for canonical, variants in enriched.items():
        if any(f["nombre"] == canonical and f["cantidad"] > 0 for f in found_items):
            continue
        if any(v in txt for v in variants):
            found_items.append({
                "nombre": canonical,
                "cantidad": 1
            })

    return found_items
